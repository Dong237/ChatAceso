# ChatAceso

<img src="assets/cover.png" width="320" alt="Aceso" align=center/>

## Contents

- [Getting Started](#getting-started)
- [Installation](#installation)
- [Finetuning](#finetuning)
- [Data](#data)
- [Inference](#inference)
- [License](#license)
- [Acknowledgements](#acknowledgements)

## Getting Started

ChatAceso is a finetuned LLM based on LLaMA-2 model from Meta

## Installation

```bash
pip install -r requirements.txt
```

## Finetuning

- Version1: the first version uses Supervised Fine-Tuning (SFT). The process uses LoRA and the script is adapted based on [alpaca-lora](https://github.com/tloen/alpaca-lora). Users can run the following example for distributed training.

  ```shell
  torchrun --nproc_per_node=4 --master_port=8888 train_lora.py \
    --base_model "meta-llama/Llama-2-7b-chat-hf" \
    --data_path '/Aceso-110k.json' \
    --output_dir './output' \
    --use_cache False \
    --batch_size 128 \
    --micro_batch_size 4 \
    --num_epochs 3 \
    --learning_rate 1e-4 \
    --cutoff_len 512 \
    --val_set_size 5000 \
    --save_strategy "steps" \
    --eval_step 200 \
    --save_step 200 \
    --logging_steps 20 \
    --report_to "wandb" \
    --wandb_project "project" \
    --wandb_run_name "run_1" \
  ```

- Version2: the second version is fine-tuned with RLAIF with the help of [RL4LMs](https://github.com/allenai/RL4LMs)

## Data

- Version1 (with medical knowledge): the dataset for version1 is a combined dataset with the open-source dataset [HealthCareMagic-100k](https://github.com/Kent0n-Li/ChatDoctor#1-chatdoctor-dataset) with the synthetic dataset - [Aceso-onboarding]-5k(https://huggingface.co/datasets/Dong237/Aceso-onboarding-5k)generated by ChatGPT

- Version2(with empathic skills) : [EmpatheticDialogues](https://huggingface.co/datasets/empathetic_dialogues)

## Inference

**Command Line Inference**

Command line inference is adapted from [OpenChatKit](https://github.com/togethercomputer/OpenChatKit/tree/main/inference). An example for starting the inference script can be found [here](https://github.com/Dong237/ChatAceso/tree/main/inference#inferencing)

**Web Demo**

ChatAceso is also available on Hugginface spaces with [Version1](https://huggingface.co/spaces/Dong237/ChatAceso-version1) and Version2

## Evaluation

## Acknowledgements
The based model of this work is from [Meta](https://ai.meta.com/llama/). My greatest appreaciation to [ChatDoctor](https://github.com/Kent0n-Li/ChatDoctor), [MedAlpaca](https://github.com/kbressem/medAlpaca), [HuatuoGPT](https://github.com/FreedomIntelligence/HuatuoGPT), [RL4LMs](https://github.com/allenai/RL4LMs) and many other similar works for providing datasets, training / evaluation frameworks and many other inspirations. 
